{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>2.165</td>\n",
       "      <td>0.681</td>\n",
       "      <td>-0.614</td>\n",
       "      <td>1.309</td>\n",
       "      <td>-0.455</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>0.276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867</td>\n",
       "      <td>1.347</td>\n",
       "      <td>0.504</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>0.672</td>\n",
       "      <td>-2.097</td>\n",
       "      <td>1.051</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>1.038</td>\n",
       "      <td>-1.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.081</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>0.317</td>\n",
       "      <td>1.172</td>\n",
       "      <td>0.352</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-1.695</td>\n",
       "      <td>-1.257</td>\n",
       "      <td>1.359</td>\n",
       "      <td>-0.808</td>\n",
       "      <td>-1.624</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>-1.099</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>0.973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.404</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-1.222</td>\n",
       "      <td>0.726</td>\n",
       "      <td>1.444</td>\n",
       "      <td>-1.165</td>\n",
       "      <td>-1.544</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.392</td>\n",
       "      <td>-1.637</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>-0.725</td>\n",
       "      <td>-1.035</td>\n",
       "      <td>0.834</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>0.640</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.467</td>\n",
       "      <td>-0.562</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>0.238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.347</td>\n",
       "      <td>-0.831</td>\n",
       "      <td>0.511</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>1.225</td>\n",
       "      <td>1.594</td>\n",
       "      <td>0.585</td>\n",
       "      <td>1.509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.134</td>\n",
       "      <td>2.415</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-1.006</td>\n",
       "      <td>1.378</td>\n",
       "      <td>1.246</td>\n",
       "      <td>1.478</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 302 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target      0      1      2      3      4      5      6      7  ...    \\\n",
       "0   0     1.0 -0.098  2.165  0.681 -0.614  1.309 -0.455 -0.236  0.276  ...     \n",
       "1   1     0.0  1.081 -0.973 -0.383  0.326 -0.428  0.317  1.172  0.352  ...     \n",
       "2   2     1.0 -0.523 -0.089 -0.348  0.148 -0.022  0.404 -0.023 -0.172  ...     \n",
       "3   3     1.0  0.067 -0.021  0.392 -1.637 -0.446 -0.725 -1.035  0.834  ...     \n",
       "4   4     1.0  2.347 -0.831  0.511 -0.021  1.225  1.594  0.585  1.509  ...     \n",
       "\n",
       "     290    291    292    293    294    295    296    297    298    299  \n",
       "0  0.867  1.347  0.504 -0.649  0.672 -2.097  1.051 -0.414  1.038 -1.065  \n",
       "1 -0.165 -1.695 -1.257  1.359 -0.808 -1.624 -0.458 -1.099 -0.936  0.973  \n",
       "2  0.013  0.263 -1.222  0.726  1.444 -1.165 -1.544  0.004  0.800 -1.211  \n",
       "3 -0.404  0.640 -0.595 -0.966  0.900  0.467 -0.562 -0.254 -0.533  0.238  \n",
       "4  0.898  0.134  2.415 -0.996 -1.006  1.378  1.246  1.478  0.428  0.253  \n",
       "\n",
       "[5 rows x 302 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['target']\n",
    "X = train.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kexu\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\kexu\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x22d345ff898>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEFCAYAAAAYKqc0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xt8VPWd//HXXDK5zeRGJoQkJEBgCCRACHjFRC0EKtpfaa0SItnabrG1i71R1tVHS/mxLGB32e1DLbra30a3CkQXa11tVVAkiiASiRAwXAIkIQm532aSzGQy5/dHIDYCGXKZnJnJ5/mPmTlzZt4jmc98c873fL4aRVEUhBBC+BWt2gGEEEKMPCnuQgjhh6S4CyGEH5LiLoQQfkiKuxBC+CG92gEuq69vJzIyhObmDrWjXJPkGzpvzgaSb7gk3/AMNZ/ZbLrmNq8auev1OrUjDEjyDZ03ZwPJN1ySb3g8kc+rirsQQoiRIcVdCCH8kBR3IYTwQ1LchRDCD0lxF0IIPyTFXQgh/JAUdyGE8ENS3IUQwg9JcRdCCD/kNe0HhBiKD4qrhv0cd6THj0ASIbyLjNyFEMIPuS3uLpeLdevWsXz5cvLy8igvL7/qY37wgx+wY8cOALq6unjkkUfIzc1l1apVNDU1jXxyIYQQ1+S2uO/ZsweHw0FBQQFr1qxhy5YtVzzmd7/7Ha2trX23d+zYgcViYfv27Sxbtoxt27aNbGohhBADcnvMvaioiMzMTADS09MpKSnpt/3tt99Go9GQlZXVb58f/OAHAGRlZV1XcY+MDAEGbmHpDSTf0Hkim8kYNOznuJzLm//fgeQbrrGWz21xt1qtGI3Gvts6nQ6n04ler+fUqVO8+eabPPnkk/z+97/vt4/J1Bs0NDSU9vZ2t0Gamzswm03U17t/rFok39B5Klu7tWvYz/Hq7lJMxqAhP9donJD15n9bkHzDNdR8A30huC3uRqMRm83Wd9vlcqHX9+72+uuvU1tby3e/+12qqqoICAggPj6+3z42m42wsLBBhxZCCDF0bot7RkYGe/fuZenSpRQXF2OxWPq2/eM//mPfz0899RTR0dFkZWVx5swZ9u3bx+zZsyksLGTevHmeSS+EEOKq3Bb37Oxs9u/fT05ODoqisGnTJvLz80lMTGThwoVX3WfFihU8+uijrFixgoCAALZu3TriwYUQQlyb2+Ku1WrZsGFDv/uSk5OveNwjjzzS93NwcDBPPvnkCMQTYuRdbOzgQr0Vh9OFy6UwaYKJlNBAtWMJMaLkClUxZnQ5nBwuredsdVu/+89Wt/HF+RbmWsYRHR6sUjohRpYUdzEmtNkcvP1JBV2OHqLCAsmwmDEGB+Bw9nC0rIkLdVbqPukg+4aJxERKgRe+T4q78Htdjh7eK7pAl6OHudOiSZ0chVar6dv+tYx46lvtvH3wPO8XXWDJTYlEmuQwjfBt0ltG+LUel4t9R6po7+gmbXIUs5LH9Svsl02JD+fWtFgcThd7Dl+go8upQlohRo4Ud+HXjpxqoLa5k6RYE3Mt0QM+Njk+nAxLNJ12J5+W1o1SQiE8Q4q78FvN7Xa+KG/GGBzAglmxaDRXjti/KnVyFOaIIMovtnOhzjoKKYXwDCnuwi8pisLB47UoCtw0Mwa97vp+1TUaDTenxqLRwCcnaul2ujycVAjPkOIu/NKZqjbqWzpJHG8k3mx0v8PfiDQFkjo5CluXk6NljR5KKIRnSXEXfqfb6eLIqXr0Og03pMQM6TlmJ48jJEhPaXkznXY5uSp8jxR34XdOVbbQ5ehhxqQoQoMDhvQcep2WtMlR9LgUTpyXxWaE75HiLvyKs8fF8XNN6HUaZiZFDuu5piWEExyo52RFC10OGb0L3yLFXfiVy6P2lKRIAg26YT2XTqclbUoUzh6F4+eaRyihEKNDirvwG/1G7ZOGN2q/zJIQTnCgjpMVzdi7e0bkOYUYDVLchd84W91Gp72H6YmRBBlGprOGTqdlxqTe0XvZhVb3OwjhJaS4C7+gKAql5c1oNDBjmMfav2pafDg6rYbSihZcijKizy2Ep0hxF37hYlMHLVYHSbEmQoJGth9eoEHHlLgwrJ3dVNXb3O8ghBeQ4i78whflLcDIj9ovS7n0vKXlcmJV+Aa3QxyXy8X69es5efIkBoOBjRs3kpSU1Lf95Zdf5rXXXkOj0fAP//AP3HnnnSiKQlZWFpMmTQIgPT2dNWvWeOxNiLGtvcPBhTor0eFBmCM804s90hTI+Khgaho7aLHaiTBKS2Dh3dwW9z179uBwOCgoKKC4uJgtW7bwzDPPANDU1MT27dt5/fXXsdvt3H333dxxxx1UVFSQmprKs88+6/E3IETppVF7iodG7ZelJEZS29TJqcoWbpwx3qOvJcRwuT0sU1RURGZmJtA7Ai8pKenbFhUVxZ///GcCAgJoaGggLCwMjUbD8ePHqa2tJS8vj1WrVnH27FnPvQMxpvX0uCirbiXIoCMp1uTR15oYYyTIoONsdRs9LmkoJryb25G71WrFaPyy8ZJOp8PpdKLX9+6q1+t56aWXeOqpp8jLywPAbDbz0EMPcdddd3H48GHWrl3Lrl27BnydyMiQS/t69gM6XJJv6DyRrbbVjqPbRcb0GCLChndIxmQMcvuYlElRFJ+qp6HNwdSECGD0/p97878tSL7hGul8bou70WjEZvtyhoDL5eor7JetXLmS+++/n1WrVnHw4EHmzJmDTtd7deD8+fOpra1FUZQB+2k3N3dgNpuor28f6nvxOMk3dJ7Kdux0PQBJ40Npt3YN+XlMxqDr2j8xJpTiU/UcO1PP+IjeL4PR+H/uzf+2IPmGa6j5BvpCcHtYJiMjg8LCQgCKi4uxWCx9286ePcvq1atRFIWAgAAMBgNarZann36aF198EYDS0lLi4uKua6EEIQajptFGbXMnE8aFYAoxjMprRhgDMUcEUd3Qga2ze1ReU4ihcDtyz87OZv/+/eTk5KAoCps2bSI/P5/ExEQWLlxISkoKy5cvR6PRkJmZyY033sj06dNZu3Yt+/btQ6fTsXnz5tF4L2KMKfy8Guht8DWapiaEU9/SRVlVK7OnDrx0nxBqcVvctVotGzZs6HdfcnJy38+rV69m9erV/baHh4fz3HPPjVBEIa7U7XSx/9hFggw6Jo4f3WOpk2LD+PSLOs5UtTEredyovrYQ10suYhI+6fMzDVg7u5kSF4ZOO7qH/AL0WpJiTVg7u7nY1DGqry3E9ZLiLnzSR8dqgN5DJGq4/LpnpJmY8FJS3IXPaW63c+xsI5MnhKl2pWhMRDBhIQFU1Frp6JITq8L7SHEXPufA8YsoCtw2K1a1DBqNhqkJ4fS4FD45UataDiGuRYq78CmKorD/WA16nZYbZ6rbAmBKXDgaDRQerVE1hxBXI8Vd+JSz1W3UNHaQYYkmNGhoi1+PlJAgPfHRoZRfbKei1nsvkBFjkxR34VMun0i9bfYElZP0unxi9XIuIbyFFHfhM+zdPRz6opZIUyAzk6LUjgNAgtmIKSSAg8drcfZIMzHhPaS4C5/x2al6Ou093JoWi3aU57Zfi1ar4ZbUWKyd3RSfblA7jhB9pLgLn/HRpROXt83yjkMyl10+RCSHZoQ3keIufEJDayel5c1MSwhnfFSI2nH6STAbmTzBxLGzjTS329WOIwQgxV34iI9LLqLgfaP2y26bHYei9M7BF8IbSHEXXs91aW67IUDL/JQYteNc1U0zYgjQa/nwaA2KoqgdRwgp7sL7na5sob6li/nTYwgOdNvIVBUhQQHMs5ipbeqgrKpN7ThCSHEX3s9bT6R+1YJLJ1Y/PFqtchIhpLgLL9dpd/LpyTqiw4OwJEaoHWdAM5IiGRcWyKHSOuyOHrXjiDFOirvwaodP1uHodnHbrAlovXypRq1Gw4JZE7A7ejh8sk7tOGKMc1vcXS4X69atY/ny5eTl5VFeXt5v+8svv8y9997Ld77zHfbu3QtAV1cXjzzyCLm5uaxatYqmpibPpBd+b/+lQzK3qtgBcjAWXDp09JE0ExMqc1vc9+zZg8PhoKCggDVr1rBly5a+bU1NTWzfvp2dO3fywgsvsH79ehRFYceOHVgsFrZv386yZcvYtm2bR9+E8E+1zR2cutDKjKRIosOD1Y5zXcwRwcxIiuRkZYus0iRU5ba4FxUVkZmZCUB6ejolJSV926Kiovjzn/9MQEAADQ0NhIWFodFo+u2TlZXFgQMHPBRf+DNfOZH6VbenxwHwwZEqlZOIscztvDKr1YrRaOy7rdPpcDqd6PW9u+r1el566SWeeuop8vLy+vYxmXoXLQ4NDaW93X071MjI3qsOzebRXex4sCTf0A0mW0+Pi49LLhIaHMCS26YQGKC76uNMxqCRijfk5/rq+1ocGcrO985w4PhFHrp3zjWzD/d1vI3kG56Rzue2uBuNRmw2W99tl8vVV9gvW7lyJffffz+rVq3i4MGD/fax2WyEhYW5DdLc3IHZbKK+3nv7Yku+oRtstiOn6mlut7MwI4G2lmsf3mi3do1EPEzGoCE/19Xe14JZsbx1oJy3Pyrj1rTh/+Xhzf+2IPmGa6j5BvpCcHtYJiMjg8LCQgCKi4uxWCx9286ePcvq1atRFIWAgAAMBgNarZaMjAz27dsHQGFhIfPmzRt0aDG2FX7eO1c8c45vHZK5LGtOHBpgrxyaESpxO3LPzs5m//795OTkoCgKmzZtIj8/n8TERBYuXEhKSgrLly9Ho9GQmZnJjTfeyKxZs3j00UdZsWIFAQEBbN26dTTei/ATze12jp5tZPIEE4njvftP6WsxRwSTNmUcx842UlHb7rPvQ/gut8Vdq9WyYcOGfvclJyf3/bx69WpWr17db3twcDBPPvnkCEUUY81Hx2pQFMicE6d2lGG5Y24cx8428v5nVTx4V4raccQYIxcxCa/iUhQ+/LwaQ4CWm2aouwD2cM1JjsYcEcSB4xdp73CoHUeMMVLchVcpLW+mobWLG1PGe22TsOul1WpYNG8i3U4X+4ql34wYXb796RF+5/KJ1CwfOiTzQfG1T5oqGoUAnZa/flJBcJAe3TWWB7wjPd5T8cQYJSN34TXaOxx8dqqeCeNCSI53P33WFxj0OqYmhNNpd1J+0Xun4gn/I8VdeI0Dx2tx9ii90wi9vEnYYKQk9Xaz/OJ8kyzkIUaNFHfhFZRLJ1J1Wg23pPlGk7DrZQoxkDjeSGObnZpG6TcjRocUd+EVTl9oparBxlyLmbAQg9pxRlzalHEAlJyVDqlidEhxF17h8pWcX5vrnycWo8ODmDAuhItNHdS3dKodR4wBUtyF6lptDg6X1hEXHcp0L19taThmyehdjCIp7kJ1hZ9X0+NSuHNuvF+dSP2q8VHBRIcHUVlnpbndrnYc4eekuAtV9bhc7CuuIjBAx61+diL1qzQaDbOn9o7ePz/ToHIa4e+kuAtVHT3TSFObnVvSYn3+itTrER8dijkiiIpaKw2tI9OuWIir8f9Pk/BaHxRXsfvTSgBMIQEDXunpLzQaDXOnmXn300qKTzewaH6C2pGEn5KRu1BNm81BTWMHMZHBRJoC1Y4zamLHhRAbFUJ1g43aZpn3LjxDirtQzcmKFgC/niFzLXOnRQPw2cl6uWpVeIQUd6EKe3cPZVWtBBl0Y3IhC3NkMInjjdS3dFFea1U7jvBDUtyFKj45UYvD6WLaxIhrdkr0dxkWM1pN7+i92+lSO47wM1LcxahTFIX3P7uARgOWieFqx1FNWKiB6YmRWDu7ea/ogtpxhJ9xO1vG5XKxfv16Tp48icFgYOPGjSQlJfVtf+GFF3jrrbcAuP322/sWzM7KymLSpEkApKens2bNGs+8A+Fzzta0UVFrJXG8kdCgALXjqGr21HGUVbfyvx+f45bU8YQbx86JZeFZbov7nj17cDgcFBQUUFxczJYtW3jmmWcAqKys5I033uDVV19Fo9GQm5vLokWLCA4OJjU1lWeffdbjb0D4nveLeqc8WiaOvROpXxUYoCN9WjSHTtRR8P4ZHvo/qWpHEn7CbXEvKioiMzMT6B2Bl5SU9G2LjY3lD3/4AzqdDgCn00lgYCDHjx+ntraWvLw8goKCeOyxx5gyZcqArxMZGQKA2ezdJ9ck39CZzSZarXY+La0j3mzEkhTlVe0GTMYgVV533oxYmtsdHDxRy92ZU0i3xFz1cd78bwuSb7hGOp/b4m61WjEajX23dTodTqcTvV5PQEAAUVFRKIrCb3/7W2bOnMnkyZNpaGjgoYce4q677uLw4cOsXbuWXbt2Dfg6zc0dmM0m6uu9d7UayTd0l7P99WA5zh4XWbMnYLV5T38VkzGIdqt6V4zmLpzGhhc/5elXitnw9zcSoNf12+7N/7Yg+YZrqPkG+kJwe0LVaDRis9n6brtcLvT6L78T7HY7v/zlL7HZbPzmN78BIC0tjYULFwIwf/58amtrZS6vwOVS2HukCkOAlgWz/LuPzGAlxZpYNG8itc2dvHWgXO04wg+4Le4ZGRkUFhYCUFxcjMVi6dumKAo//vGPmT59Ohs2bOg7PPP000/z4osvAlBaWkpcnH8tmyaG5tjZRhpau7h55nhCxviJ1KtZljmZSFMgfzlYzsUmuXJVDI/bwzLZ2dns37+fnJwcFEVh06ZN5Ofnk5iYiMvl4tChQzgcDj788EMAfvGLX/DQQw+xdu1a9u3bh06nY/PmzR5/I8L7vf/ZpQU5MqSfytUEB+pZsXAa214v4Y/vnOSXOekyKBJD5ra4a7VaNmzY0O++5OTkvp+PHTt21f2ee+65YUYT/qSmwUbJ2UamxoePyStSr9e86WZmJ4/jaFkjn5yo5eZUOXwlhka6QoohGWwHx6NlTSj0Ns0aC90fh0qj0fBAtoXS8k/Y8d5p0qaMwxgsh7DE4MkVqsLjnD0uvjjfRJBBR1Ks0f0OY5w5IphvZk6mvaObHXtOqR1H+Cgp7sLjztW0Y+/uYVpCODqt/Mpdj8U3TGRSrIkDx2s5WiarNonBk0+a8ChFUThZ0YwGuSJ1MHRaLd9bOgOdVsN/v3OSjq5utSMJHyPFXXhUQ2sXTW12JseFEyrHjgdlYoyRpTcn0dRm54W3TqgdR/gYKe7Coy4vyJGWPE7lJL7pnlsnERcdyl8/Ps/Jima14wgfIsVdeEyn3cn5mnbCQg0kxMiJ1KEI0Gv53l0paDTwwl9LcXT3qB1J+Agp7sJjzlxoxaUoTJ8YIRfjDENyfDj/JzOZ2uZO/vzRObXjCB8hxV14hEtROFXZgl6nITk+TO04Pm/l11OIDg/i7UMVnKtpUzuO8AFS3IVHXKizYutyMiUuDEOAzv0OYkBBgXoevCsFRYH8v5Ti7JFl+cTApLgLj7h8InV6YqTKSfzHzElRZM2ZwIV6K389KJ0jxcCkuIsR12ZzUNPYQUxkMJEmWTZuJN1/51QijAb+9+Pz0jlSDEiKuxhxX47a5aKlkRYSFEDuIgvOHoWXd5+SdRLENUlxFyOq2+niTFUrwYE66f7oIfOmm0mdHMXxc00UnaxXO47wUlLcxYg6V9NGt9PFtIQIdFqZ/ugJGo2GldkW9DoNO947TZfDqXYk4YWk5a8YUacrW9EA0yaGqx3Fp7hrg3y1NV5nTIriWFkjv//TMeZNj+GO9HhPRhQ+RkbuYsQ0tnXR2NZFvDmUUFlGz+NmTYnCGBzAifPNtLR7z2Ljwju4Le4ul4t169axfPly8vLyKC/vPwXrhRde4L777uO+++7j6aefBqCrq4tHHnmE3NxcVq1aRVNTk2fSC69yurIVkO6Po0Wv03LDjBgUBT45IYvQi/7cFvc9e/bgcDgoKChgzZo1bNmypW9bZWUlb7zxBjt37qSgoICPPvqI0tJSduzYgcViYfv27Sxbtoxt27Z59E0I9XU7XZyrbiMkUE9cdKjaccaMiTFGEsyh1DZ3cvBErdpxhBdxW9yLiorIzMwEID09nZKSkr5tsbGx/OEPf0Cn06HVanE6nQQGBvbbJysriwMHDngovvAW5y+2093jYmpCOFo5kTqqbpgRg06r4ZW9Z7A7pLGY6OX2hKrVasVo/LKjn06nw+l0otfrCQgIICoqCkVR+O1vf8vMmTOZPHkyVqsVk6l3GlxoaCjt7e1ug0RGhgBgNnv39DnJ18tkDOp3+2x1JQDp02MwhRiuax9v46v5TMYg5lrMHC6to7DkIrlLUkY5WS/5bAzPSOdzW9yNRiM2m63vtsvlQq//cje73c7jjz9OaGgov/nNb67Yx2azERbmvnFUc3MHZrOJ+nr3XwRqkXxf+tuZG83tXdQ2dRBvDgWX64pZHXD12R7exNfzTUsI53RlC7v2nmb+tOhRvzJYPhvDM9R8A30huD0sk5GRQWFhIQDFxcVYLJa+bYqi8OMf/5jp06ezYcMGdDpd3z779u0DoLCwkHnz5g06tPAdpy6dSJ2WINMf1RKg1/KtrCk4ul38qfCs2nGEF3A7cs/Ozmb//v3k5OSgKAqbNm0iPz+fxMREXC4Xhw4dwuFw8OGHHwLwi1/8ghUrVvDoo4+yYsUKAgIC2Lp1q8ffiFCHs8fF2eo2ggN1JJhlQQ413TZrAnsOV7L/WA2L5ifIFcJjnNvirtVq2bBhQ7/7kpOT+34+duzYVfd78sknhxlN+ILyi+10O12kJEbJiVSVabUa7r9zKv/+yue8VniWn903R+1IQkVyEZMYlsuHZKbKIRmvkDo5ipTECI6WNXKqskXtOEJFUtzFkLW026lv6WTCuJBrzpARo0uj0XDv7b1/Wf/PvjK5sGkMk+Iuhuz0Bbki1Rslx4czd1o0Zy608vmZRrXjCJVIcRdD0tPjoqy6lSCDjokxciLV23w7awoa4PWPzsrofYySrpBiSMprrTi6XaRNlhOp3uKrnSWTYk2cv9jOy7tPkXCdX8DSWdJ/yMhdDMnpSyfrpLWv95qdPA6Az8saZfQ+BklxF4NW02ijtrmTWDmR6tUiTIEkjTfS2NpFdYOstzrWSHEXg7avuBoAi0x/9HqzLo/ezzTI6H2MkeIuBqXb6eLjkou9J1LlCkivFxUWxMQYIw2tXdQ0yuh9LJHiLgbls1P1WDu7mRIXJmuk+oi+Y+9n5Nj7WCLFXQzKvkszMmRuu+8YFx5EgjmU+pZOaps61Y4jRokUd3Hdaps6KK1oISUxgrBQOZHqS2ZPvTxzpkHlJGK0SHEX163w894TqVlz4lROIgYrOjyY+OhQaps6qW2SY+9jgRR3cV2cPS4+OlZDaJCeedPNascRQ3B55kzJOVmwfiyQ4i6uS9HJeto7ulkwawIBep3accQQxEQGY44IoqreRovVrnYc4WFS3MV12Xuk90TqnXPl8nRfljo5CoDjMnr3e1LchVtV9VZOVbYwc1Ik46NC1I4jhmFijJGwUAPnqtvo6HKqHUd4kNvi7nK5WLduHcuXLycvL4/y8vIrHtPU1MTixYux23v/1FMUhczMTPLy8sjLy5Nl9nzcB0d6T6TKqN33aTQaZk6KxKXAF+XNascRHuS2K+SePXtwOBwUFBRQXFzMli1beOaZZ/q2f/jhh2zdupWGhi+nWFVUVJCamsqzzz7rmdRi1HQ5nHx8vIYIo4H0adFqxxEjIDkujOLTDZyqbGFWchQGOYfil9yO3IuKisjMzAQgPT2dkpKS/k+g1ZKfn09ExJcXtRw/fpza2lry8vJYtWoVZ8/Kauy+6pMTtXTae8iaE4dOK0fx/IFOpyUlKZJup4vTl5ZJFP7H7cjdarViNH7ZC1qn0+F0OtHre3ddsGDBFfuYzWYeeugh7rrrLg4fPszatWvZtWvXgK8TGRlyaV/v7lcylvIpisKHxy6i1Wr49kIL48KD+7aZjEGDfr6h7DOaxlK+eTPGU3K2iZMVLdyQOqGvlcRwfn/G0mfDE0Y6n9vibjQasdlsfbddLldfYb+WtLQ0dLreP/Xmz59PbW0tiqKg0Vy7F0lzcwdms4n6+vbrzT7qxlq+supWzla1kmEx43I4+z13u7VrUM9lMgYNep/RNBbzTUsI54vyZo6driM5vrfD51B/f8baZ2OkDTXfQF8Ibv/OzsjIoLCwEIDi4mIsFovbF3z66ad58cUXASgtLSUuLm7Awi680wefyfRHfzZjUiQaTe+0SGko5n/cjtyzs7PZv38/OTk5KIrCpk2byM/PJzExkYULF151n4ceeoi1a9eyb98+dDodmzdvHvHgwrOsnd0cKq0jJjKYGZMi1Y4jPMAYHNC7FF9NO9UNHcSbQ9WOJEaQ2+Ku1WrZsGFDv/uSk5OveNz777/f93N4eDjPPffcCMQTatl/rIZup4s70uPRyl9dfit1UhTna9o5cb5JirufkekP4goul8Lez6rQ67TcNnuC2nGEB40LD2J8VDA1jR00t3vvOQcxeFLcxRU+P9NAXUsnt6SOxxgcoHYc4WGpk3pbEpw4Jxc1+RMp7uIKuw9XApB9w0SVk4jREG8O7W1JUNNGc7s0FPMXUtxFPxW17ZRWtJA6KZIEs9H9DsLn/W1Lgvc/u6B2HDFCpLiLft799PKoPVHlJGI0TYkLI8ig44MjVdgdPWrHESNAirvo02K188mJWiaMCyFtSpTaccQo0uu0WCZGYOty8tGxGrXjiBEgxV30efdQJT0uhcU3TJTpj2PQ9MQI9Dot735agcslFzX5OinuAui9aGlvcRURRgO3psn0x7EoOFDPrWmx1Ld0ceR0vdpxxDBJcRcAvF90AbujhyU3JhKgl1+LsWrJjb0zpN45VKlyEjFc8ikWdDmc7D5cSWiQntvT49SOI1Q0YVwos5PHcaaqlbIqaQfsy6S4C/YVV2PrcrJo/kSCDG47Ugg/t+TG3plS7xyqUDmJGA4p7mNcl8PJXw6WE2TQsXBegtpxhBdISYwgcbyRolP11Ld0qh1HDJEU9zFuz+ELtHd0s/iGidJqQAC9FzUtuTERRYHdn8qxd18lf4OPUR8UV+Ho7uHNj89jCNASEqzng+IqtWMJL3FDSgz/80EZHx6t4ZuZkwkNki9+XyMj9zHs+PlmHE4XaZNlkWTRn16nZdH8BOzdPewrrlY7jhgCKe5jVEeXky/caYYOAAAVqElEQVTONxEcqCMlSRbjEFe6fU4cgQYdew5X4uxxqR1HDJIU9zHqyOl6nD0Kc6ZGo9fJr4G4UkhQAFmz42ixOjj0Ra3accQgyad6DDpX00ZZVRuRpkCmJoSrHUd4sez5CWg0vRc1yTqrvsVtcXe5XKxbt47ly5eTl5dHeXn5FY9pampi8eLF2O29vaC7urp45JFHyM3NZdWqVTQ1NY18cjEkiqKwfc8pAG6YESM9ZMSAoiOCmT89hso6K1+Uy2IevsRtcd+zZw8Oh4OCggLWrFnDli1b+m3/8MMP+f73v09DQ0PffTt27MBisbB9+3aWLVvGtm3bRj65GJIDxy9SVtVG4ngjsVEhascRPuDyRU1vfyIXNfkSt1Mhi4qKyMzMBCA9PZ2SkpJ+27VaLfn5+dx777399vnBD34AQFZW1nUV98jI3kJjNpuuP70KfDlfq9VOwftlBBp03J4xEVOoYRSTgckYNKqvN1iS7+q/P2azibTkc5SUNdJq72FqQsR17+tNxlo+t8XdarViNH65Io9Op8PpdKLX9+66YMGCq+5jMvUGDQ0Npb293W2Q5uYOzGYT9fXuH6sWX8/33P8ep73DQc7CaWgUF+3W0VsQ2WQMGtXXGyzJ1+tavz+L5ydQUtbIS385wT98a9YV2339s6G2oeYb6AvB7WEZo9GIzWbru+1yufoK+/XsY7PZCAsLu96swkOOljVy8HgtkyeEsUjaDIhBSp0UxaRYE5+drKe6weZ+B6E6tyP3jIwM9u7dy9KlSykuLsZisbh90oyMDPbt28fs2bMpLCxk3rx5IxJW9LrWlaTXGt3ZHT387/7zaDSQNiWKwqNyUYoYHI1Gw923TOL3fzrGXw6W84N7ZqodSbjhduSenZ2NwWAgJyeHzZs389hjj5Gfn8977713zX1WrFjB6dOnWbFiBQUFBaxevXpEQ4vrpygKB45fpMPuZM7UaCJNgWpHEj5qriWa+OhQDh6vpU4aink9tyN3rVbLhg0b+t2XnJx8xePef//9vp+Dg4N58sknRyCeGK4zF1qpqLUyPjJY1kUVw6LVaLj71iSee+MEb+4/z/fvnqF2JDEAuYjJj7W02/m0tA6DXsttsyfInHYxbDemjCcuOpSPSy5S29ShdhwxAOkK6acc3T3sPVKFs0fh9vQJhEo7X3Edrqcz6NSEcKobbDz/5glum9273u7l8z13pMd7OqK4TjJy90OKovDR0RraO7pJnRxFUqx3z+8VviVpvJEIo4Fz1W20Wh1qxxHXIMXdDxWfbuBCvY0J40KYa4lWO47wMxqNhvRp0ShA8ZkGt48X6pDi7mfOXGjl2NkmjMEBZM6Jk+PswiMmxhgZFx5E+cV2GmTmjFeS4u5HKmvbOXD8IoYALQvnJRBkkAU4hGdoNBrmTTcDUHSyXjpGeiEp7n6isa2Ltw+cR4OGO+fGE24c3b4xYuyJjQohwRxKbXMn5TXee2n/WCXF3Q+02Ry8d/gCDqeLBbNjGS/dHsUoybCY0QAfl1Tjcsno3ZtIcfdxtq5udn9aSZejh9vnxjN5gvTxEaMn4tKCL81tdk5WtKgdR/wNKe4+rKPLybuHKrF1OUmfOo60ZJkZI0bfXEs0gQE6is800GqTqZHeQoq7j+ot7BW0d3STNiWKWcnj1I4kxqggg56bUmPpdrr4nw/OqB1HXCLF3Qd12p3s/rSStksXKc2dFo1GpjwKFaVOGUekKZD9xy5y+oIcnvEGUtx9TJvNwbufVtJqczBzUiQZFinsQn1arYabZsagAfL/Ukq3s0ftSGOeFHcf0tbh4F93HqHV6mBGUiTzppulsAuvERMZwtfmJXCxqYM39p9XO86YJ8XdR7R3OPi3HcVU1dtISYxgfooUduF97r19CtHhQfz1YAXnL7apHWdMk+LuA6yd3WzdWcyFeit3ZsRzw4wYKezCKwUZ9Dx4VwouReEPb36BvVsOz6hFiruXs3V18287j1BRZ+WO9DgeyLZIYRdebeakKBZmJFDdYGPne6fVjjNmue3n7nK5WL9+PSdPnsRgMLBx40aSkpL6tr/yyivs3LkTvV7Pww8/zJ133klLSwtLlizpW2910aJFfPe73/Xcu/BTHV3d/NvOYipqrWTNmcDKJdOlEZjwCfd/LZlTF1rYV1zNzElR3JASo3akMcdtcd+zZw8Oh4OCggKKi4vZsmULzzzzDAD19fX88Y9/ZNeuXdjtdnJzc1mwYAEnTpzgnnvu4de//rXH34C/6ujqZmvB55RfbOe2WRP4u6+nSGEXPiNAr+NH30zl/77wKS/8tZTEGKO0xRhlbot7UVERmZmZAKSnp1NSUtK37ejRo8ydOxeDwYDBYCAxMZHS0lJKSko4fvw4K1euJCoqil/96lfExAz8zR0Z2fsPbzZ798ISo5GvzebgX14q4lxNG1+bP5GfLJ+LTvtlYTcZg66570Db1ObN2UDyDZfJGNTv82E2m/jxvXP43c4jbPtzCf/6SJaqK4KNtdritrhbrVaMRmPfbZ1Oh9PpRK/XY7VaMZm+DBQaGorVamXKlCmkpaVx66238sYbb7Bx40a3C2Y3N3dgNpuor/fe7nKjka/t0qyYC/W9h2JyF06lqdHa7zHt1q6r7nt5qTNv5M3ZQPIN1+V8X/18zJ4USfb8iew+XMmm/E/4yb2z0WpH/y9Qf60tA30huC3uRqMRm83Wd9vlcqHX66+6zWazYTKZmD17NsHBwQBkZ2e7LeyiV6vVzr/uLKa6wcadc+N5YLFFDsUIn3K1NVjHRwUzYVwIR8sa2VpQ7HYar6zDOjLczpbJyMigsLAQgOLi4r6TpACzZ8+mqKgIu91Oe3s7ZWVlWCwWfvWrX/HOO+8AcODAAVJTUz0U3380t9t5YvsRqhtsLJqfwEop7MJPaLUastLjCA818EV5M8fONqkdaUxwO3LPzs5m//795OTkoCgKmzZtIj8/n8TERBYuXEheXh65ubkoisLPf/5zAgMDWbNmDY8//jg7duwgODiYjRs3jsZ78VlNbV38dscR6po7+fpNidx3R7JMdxR+JTBAx6IbEnj7YAXFpxsIDNAyPTFS7Vh+TaN4yfpY9fXtfntcbCANLZ38dscRGlq7uOfWJL6VOcVtYb/an77g3cdlvTkbSL7hut58bTYHb39SQZejhxtnxJCSdGWB98RhGX+tLQMdc5eLmFRU02jjie2f0dDaxbLbJvPtLBmxC/8WFmog+4aJBBl0HPqijuPn5BCNp7g9LCNG1uVRd0NLJ+8VVWHv7iHDEk2Y0XDNEbkQ/iTSFMiSGxPZ/WklRSfr6XI4e5frk4HNiJKRuwqq6m28+2klju4ebkkdT9oUWWhDjC3hRgNLbppIWEgAx881s6+4GmePS+1YfkWK+yg7W93G+59dQFHg9rlxTJsYoXYkIVRhCjFw181JjI8MpqLWytufVNDeIcv0jRQp7qNEURTePVTBR0dr0Ou0LJqfQOJ4775iTghPCzT0zqKZmhBOU5udtz4up/h0g9qx/IIU91HQ7XSR/9dSdr5/huBAHV+/KVH6bAhxiU6r5da0WG5Ji6XHpfDkrqP88d2T2B3SLng45ISqhzW32/n9n45xtrqNpFgT86ebVe2vIYS3mpYQzriwQI6camDvZ1WcONfE9++ewbQEOXQ5FDJy96CyqlY2vPgpZ6vbuCV1PI89kCGFXYgBRIUFse7B+Sy+YSJ1zZ1sfukz/vvtUmxd3WpH8zkycvcARVHY93k123efoselsPxrU1l8w0SZ6iXEdQjQ68hZOI3502N48e1SPiiupuhUPd/KnELWnDhVGo/5IinuI8za2c2Lfy2l6FQ9oUF6fvTNNFInR6kdSwifMzUhnN987wbeOVTBmx+X89/vnOT9zy7wzdumMNcSLb2X3JDiPoKKzzTwx3dO0txuxzIxglX3zGRcuHf34BbCm+l1Wu6+ZRILZk3gtcKz7D9aw+//dIx4cyjfuHUS86fHyEj+GqS4j4BWq50d753m0Bd16LQavp01haU3J8kvnRAjJMIYyPeXzuCumxJ58+NyPjlRy7N/Ps6EcedYenMSN86IIUCvUzumV5HiPgzdThe7D1fy5sfn6XL0kBwXxoN3pRBvNrrfWQgxaBPGhbLqGzP55m2TeOtAOR+XXOT/vfUFO987za1pE8hKjyM+OlTtmF5BivsQOHtcfFxykf/df57Gti6MwQHkLU7m9vR4Ga0LMQpiIkP43tIZfGPBJPYeqWL/0Rp2H65k9+FKpiaEc9usCaRPiyYsxKB2VNVIcR8Ea2c37x6qYE/RBRpau9DrtCy+YSLfWDCJ0CCZ4ijEaIsOD+a+O6byrcwpFJ9uYN/n1Zw418SZC61o3gZLQgQZFjOLbpnEWBt2SXF3w6UonKpo4cDxi3xaWkeXo4cAvZZF8xK46+YkIk2BakcUwq8MpzvqvOlmpk+MQK/TUnSqjpOVLZysbGHHe6eJjw5lemIEKYmRWBIj/H5UL8X9Kjq6nJysbOZoWSOfn2mgxdrbzMgcGcw9t8aRNScOo1yMJIRXMoYEcEd6PF+/KZEWq50jpxsoOdfE8bONVDXYeP+z3i+P+OhQJk8II3G8kaRYExNjjAQZ/Kckun0nLpeL9evXc/LkSQwGAxs3biQpKalv+yuvvMLOnTvR6/U8/PDD3HnnnTQ1NfHLX/6Srq4uYmJi2Lx5c9+C2d7EpSi02Rw0tHRRWW+lss5KWVUrF+qsXF6eKjRIz22zJ3BraiwLMibS2GhVNbMQ4vpFGAO5c2489y9OoeZiK+dq2iitaOFkRTNnLrRS1WCDY72P1QAxkcGMjwrp/W9kCOMjg4mOCCY81EBwoG8Vfrdp9+zZg8PhoKCggOLiYrZs2cIzzzwDQH19PX/84x/ZtWsXdrud3NxcFixYwLZt27jnnnv49re/zXPPPUdBQQEPPvigR95AR1c3ze12nD0KTpeLnh6F7h4XPT0unD0KdkcPtq5uOuxOOrqc2C49vrG1i8Y2+xU9pPU6LdMmRmCZGMHsKeOYEhfWd5JUTpYK4bv0Oi3TEiKYlhDBN26dRI/LxcXGDipqrZTXtlNR205lnZXassar7h8YoCPcaCAi1IApxEBQoI5gg56gQD3BfT/rCArQo9Np0Gk16HVadFoNOp0GvVaLVqvh8rVXGo0GDb297T3yft09oKioiMzMTADS09MpKSnp23b06FHmzp2LwWDAYDCQmJhIaWkpRUVF/PCHPwQgKyuLf//3f/dIcXf2uPjHZw7QYXcOel9TSAAJ5lDGhQcRHR5EfLSRiTFG4qJDCdBLyx0h/J1OqyXebCTebOSWtNi++62d3dS3dFLb3EFdcycNrV20Wh20Wu202Bycbm5lJBeejjQF8uJvlozgM/ZyW9ytVitG45fztnU6HU6nE71ej9VqxWT6sid5aGgoVqu13/2hoaG0t7tf+PXyQq8DLfh6NQWb7h7U44drsPm+6r7slBFKIoQYjOv97JqByYmezXLV1x1mbfkqt0NUo9GIzWbru+1yudDr9VfdZrPZMJlM/e632WyEhYWNaGghhBADc1vcMzIyKCwsBKC4uBiLxdK3bfbs2RQVFWG322lvb6esrAyLxUJGRgb79u0DoLCwkHnz5nkovhBCiKvRKIoy4OGjy7NlTp06haIobNq0icLCQhITE1m4cCGvvPIKBQUFKIrCD3/4Q5YsWUJDQwOPPvooNpuNyMhItm7dSkiIrDwkhBCjxW1xF0II4XtkWogQQvghKe5CCOGHpLgLIYQf8sriXlZWxrx587Db7WpH6aejo4OHH36Y3Nxc/v7v/56mpia1I/Vpb2/nRz/6EStXrmT58uUcOXJE7UhXtXv3btasWaN2jD4ul4t169axfPly8vLyKC8vVzvSFT7//HPy8vLUjnGF7u5u1q5dS25uLt/5znd477331I7UT09PD4899hg5OTk88MADVFRUqB3pqhobG7n99tspKysb0ef1uuJutVp54oknMBi8r2PbK6+8QmpqKtu3b+fuu+9m27Ztakfqk5+fz80338xLL73E5s2b2bBhg9qRrrBx40a2bt2Ky+Vy/+BR8rftNdasWcOWLVvUjtTP888/z69+9SuvG+gAvPHGG0RERLB9+3aef/55/vmf/1ntSP3s3bsXgJ07d/KTn/yEzZs3q5zoSt3d3axbt46goJFfjtOriruiKPz617/mF7/4hVc2GnvwwQd5+OGHAaiuriY6OlrlRF968MEHycnJAXpHLIGB3teKOCMjg/Xr16sdo5+B2mt4g8TERJ566im1Y1zV17/+dX7605/23dbpvGuZu0WLFvV94Xjb5/WyJ554gpycHGJiYkb8uVVrc/bqq6/y4osv9rsvLi6OpUuXkpKi/iX6V8u3adMmZs+ezd/93d9x6tQp8vPzvS5bfX09a9eu5fHHH1clG1w739KlS/nkk09USnV1A7XX8AZLlizhwoULase4qtDQ3uXsrFYrP/nJT/jZz36mcqIr6fV6Hn30UXbv3s2TTz6pdpx+XnvtNaKiosjMzOS5554b+RdQvMiiRYuUlStXKitXrlTS0tKU3NxctSNd05kzZ5SFCxeqHaOf0tJSZenSpcoHH3ygdpRrOnjwoPKzn/1M7Rh9Nm3apLz11lt9tzMzM1VMc3WVlZXKfffdp3aMq6qurla+9a1vKa+++qraUQZUV1en3HHHHYrNZlM7Sp/c3FzlgQceUFauXKnMmzdPuffee5W6uroRe37vGJ5csnv37r6fv/a1r/Ff//VfKqa50n/+538yfvx4li1bRkhIiFf9GXrmzBl++tOf8rvf/c4r/vLxFRkZGezdu5elS5de0V5DDKyhoYHvf//7rFu3jltuuUXtOFd4/fXXqa2t5Yc//CHBwcFoNBqv+sy+/PLLfT/n5eWxfv16zGbziD2/VxV3b3fvvffy6KOPsmvXLnp6eti0aZPakfps3boVh8PBv/zLvwC9Td0u990X15adnc3+/fvJycnpa68hrs+zzz5LW1sb27Zt65tc8Pzzz3vk5OBQLF68mMcee4wHHngAp9PJ448/7pXnojxF2g8IIYQf8qrZMkIIIUaGFHchhPBDUtyFEMIPSXEXQgg/JMVdCCH8kBR3Idx44okn+Kd/+ie1YwgxKFLchRjAgQMH+NOf/qR2DCEGTYq7ENfQ0tLCf/zHf/CjH/1I7ShCDJoUdyGuYd26dfz85z8nLCxM7ShCDJoUdyGu4tVXX2XChAle2TNFiOsh7QeEuIrvfe971NfXo9PpaG1tpaOjg2XLlqnaSlmIwZDiLoQbr732GocOHfK6VZqEGIgclhFCCD8kI3chhPBDMnIXQgg/JMVdCCH8kBR3IYTwQ1LchRDCD0lxF0IIPyTFXQgh/JAUdyGE8EP/H1puKkK7TNF0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "sns.distplot(X['4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "selector = SelectKBest(f_classif, k=10)\n",
    "selected_features = selector.fit_transform(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_score_indexes = (-selector.scores_).argsort()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_score_indexes.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X.iloc[:,f_score_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>4</th>\n",
       "      <th>13</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>24</th>\n",
       "      <th>30</th>\n",
       "      <th>33</th>\n",
       "      <th>39</th>\n",
       "      <th>43</th>\n",
       "      <th>...</th>\n",
       "      <th>237</th>\n",
       "      <th>239</th>\n",
       "      <th>244</th>\n",
       "      <th>252</th>\n",
       "      <th>258</th>\n",
       "      <th>272</th>\n",
       "      <th>276</th>\n",
       "      <th>289</th>\n",
       "      <th>295</th>\n",
       "      <th>298</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.098</td>\n",
       "      <td>1.309</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.673</td>\n",
       "      <td>1.763</td>\n",
       "      <td>1.125</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.966</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.754</td>\n",
       "      <td>-0.516</td>\n",
       "      <td>-1.516</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.569</td>\n",
       "      <td>-2.097</td>\n",
       "      <td>1.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.081</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>1.540</td>\n",
       "      <td>1.098</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>-1.519</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>-2.721</td>\n",
       "      <td>-0.303</td>\n",
       "      <td>-1.320</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.254</td>\n",
       "      <td>0.366</td>\n",
       "      <td>-1.160</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.630</td>\n",
       "      <td>-0.516</td>\n",
       "      <td>2.064</td>\n",
       "      <td>-0.612</td>\n",
       "      <td>-1.624</td>\n",
       "      <td>-0.936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.523</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.352</td>\n",
       "      <td>-1.044</td>\n",
       "      <td>0.270</td>\n",
       "      <td>1.786</td>\n",
       "      <td>-2.032</td>\n",
       "      <td>0.924</td>\n",
       "      <td>1.920</td>\n",
       "      <td>1.088</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.234</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>2.342</td>\n",
       "      <td>0.678</td>\n",
       "      <td>1.459</td>\n",
       "      <td>0.346</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-1.165</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>-1.010</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.394</td>\n",
       "      <td>-2.241</td>\n",
       "      <td>0.423</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.507</td>\n",
       "      <td>1.176</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>0.530</td>\n",
       "      <td>2.725</td>\n",
       "      <td>1.527</td>\n",
       "      <td>-1.618</td>\n",
       "      <td>0.467</td>\n",
       "      <td>-0.533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.347</td>\n",
       "      <td>1.225</td>\n",
       "      <td>1.478</td>\n",
       "      <td>-1.312</td>\n",
       "      <td>-0.322</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.308</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>...</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.906</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>1.845</td>\n",
       "      <td>1.378</td>\n",
       "      <td>0.428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      4     13     16     17     24     30     33     39     43  \\\n",
       "0 -0.098  1.309  0.102  0.177 -0.673  1.763  1.125  0.385  0.005  0.579   \n",
       "1  1.081 -0.428  1.540  1.098 -0.237 -1.519 -0.227 -2.721 -0.303 -1.320   \n",
       "2 -0.523 -0.022  0.352 -1.044  0.270  1.786 -2.032  0.924  1.920  1.088   \n",
       "3  0.067 -0.446 -1.010  0.210  0.836  0.365  0.999  0.394 -2.241  0.423   \n",
       "4  2.347  1.225  1.478 -1.312 -0.322  0.024 -0.308  0.037 -0.122 -0.318   \n",
       "\n",
       "   ...      237    239    244    252    258    272    276    289    295    298  \n",
       "0  ...    0.966 -0.181  0.074  0.754 -0.516 -1.516  0.820  0.569 -2.097  1.038  \n",
       "1  ...   -2.254  0.366 -1.160  0.960  0.630 -0.516  2.064 -0.612 -1.624 -0.936  \n",
       "2  ...   -1.234 -0.010  2.342  0.678  1.459  0.346 -0.208  0.202 -1.165  0.800  \n",
       "3  ...    0.054  0.507  1.176 -0.497  0.530  2.725  1.527 -1.618  0.467 -0.533  \n",
       "4  ...    0.159  0.024 -0.233 -0.191 -0.906 -0.464 -0.523  1.845  1.378  0.428  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X1, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=40, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=40)\n",
    "neigh.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = neigh.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875620347394541"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "predgnb = gnb.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9416873449131513"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, predgnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250</td>\n",
       "      <td>0.500</td>\n",
       "      <td>-1.033</td>\n",
       "      <td>-1.595</td>\n",
       "      <td>0.309</td>\n",
       "      <td>-0.714</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.535</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.687</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-2.628</td>\n",
       "      <td>-0.845</td>\n",
       "      <td>2.078</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>2.132</td>\n",
       "      <td>0.609</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>251</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.914</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>1.347</td>\n",
       "      <td>-0.867</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.578</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>0.203</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.683</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.606</td>\n",
       "      <td>-0.353</td>\n",
       "      <td>-1.133</td>\n",
       "      <td>-3.138</td>\n",
       "      <td>0.281</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>-0.761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>252</td>\n",
       "      <td>1.750</td>\n",
       "      <td>0.509</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.835</td>\n",
       "      <td>-0.476</td>\n",
       "      <td>1.428</td>\n",
       "      <td>-0.701</td>\n",
       "      <td>-2.009</td>\n",
       "      <td>-1.378</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.351</td>\n",
       "      <td>-0.607</td>\n",
       "      <td>-0.737</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-1.327</td>\n",
       "      <td>2.463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>253</td>\n",
       "      <td>-0.556</td>\n",
       "      <td>-1.855</td>\n",
       "      <td>-0.682</td>\n",
       "      <td>0.578</td>\n",
       "      <td>1.592</td>\n",
       "      <td>0.512</td>\n",
       "      <td>-1.419</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.511</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>-0.787</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.836</td>\n",
       "      <td>0.916</td>\n",
       "      <td>2.411</td>\n",
       "      <td>1.053</td>\n",
       "      <td>-1.601</td>\n",
       "      <td>-1.529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>254</td>\n",
       "      <td>0.754</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>1.173</td>\n",
       "      <td>-1.623</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.781</td>\n",
       "      <td>-1.763</td>\n",
       "      <td>-1.432</td>\n",
       "      <td>...</td>\n",
       "      <td>2.184</td>\n",
       "      <td>-1.090</td>\n",
       "      <td>0.216</td>\n",
       "      <td>1.186</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.322</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-1.153</td>\n",
       "      <td>0.825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id      0      1      2      3      4      5      6      7      8  ...    \\\n",
       "0  250  0.500 -1.033 -1.595  0.309 -0.714  0.502  0.535 -0.129 -0.687  ...     \n",
       "1  251  0.776  0.914 -0.494  1.347 -0.867  0.480  0.578 -0.313  0.203  ...     \n",
       "2  252  1.750  0.509 -0.057  0.835 -0.476  1.428 -0.701 -2.009 -1.378  ...     \n",
       "3  253 -0.556 -1.855 -0.682  0.578  1.592  0.512 -1.419  0.722  0.511  ...     \n",
       "4  254  0.754 -0.245  1.173 -1.623  0.009  0.370  0.781 -1.763 -1.432  ...     \n",
       "\n",
       "     290    291    292    293    294    295    296    297    298    299  \n",
       "0 -0.088 -2.628 -0.845  2.078 -0.277  2.132  0.609 -0.104  0.312  0.979  \n",
       "1 -0.683 -0.066  0.025  0.606 -0.353 -1.133 -3.138  0.281 -0.625 -0.761  \n",
       "2 -0.094  0.351 -0.607 -0.737 -0.031  0.701  0.976  0.135 -1.327  2.463  \n",
       "3 -0.336 -0.787  0.255 -0.031 -0.836  0.916  2.411  1.053 -1.601 -1.529  \n",
       "4  2.184 -1.090  0.216  1.186 -0.143  0.322 -0.068 -0.156 -1.153  0.825  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= test.drop('id',axis=1)\n",
    "test = test.iloc[:,f_score_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1  = gnb.predict_proba(test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = neigh.predict_proba(test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['target'] = (result2+result1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250</td>\n",
       "      <td>0.750298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>251</td>\n",
       "      <td>0.816598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>252</td>\n",
       "      <td>0.747260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>253</td>\n",
       "      <td>0.899977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>254</td>\n",
       "      <td>0.781184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id    target\n",
       "0  250  0.750298\n",
       "1  251  0.816598\n",
       "2  252  0.747260\n",
       "3  253  0.899977\n",
       "4  254  0.781184"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('submission2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.935483870967742"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(gamma='auto',probability=True)\n",
    "clf.fit(X_train, y_train)\n",
    "predsvc = clf.predict_proba(X_test)[:,1]\n",
    "roc_auc_score(y_test, predsvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3 = clf.predict_proba(test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['target'] = (result1 + result2+ result3)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('submission3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9100496277915633"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clfr = RandomForestClassifier(n_estimators=200, max_depth=10,\n",
    "                             random_state=0)\n",
    "clfr.fit(X_train, y_train)\n",
    "predrf = clfr.predict_proba(X_test)[:,1]\n",
    "roc_auc_score(y_test, predrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "result4 = clfr.predict_proba(test)[:,1]\n",
    "result['target'] = (result1 + result2 + result3 + result4) /4 \n",
    "result.to_csv('submission4.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.880893\n",
      "[2000]\tvalid_0's auc: 0.89268\n",
      "[3000]\tvalid_0's auc: 0.884615\n",
      "Early stopping, best iteration is:\n",
      "[2257]\tvalid_0's auc: 0.894541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8945409429280398"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "d_train = lgb.Dataset(X_train, label=y_train)\n",
    "d_val = lgb.Dataset(X_test, label = y_test)\n",
    "params = {}\n",
    "params['learning_rate'] = 0.01\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['objective'] = 'binary'\n",
    "params['metric'] = 'auc'\n",
    "params['sub_feature'] = 0.5\n",
    "params['num_leaves'] = 100\n",
    "params['min_data'] = 20\n",
    "params['max_depth'] = 20\n",
    "clfl = lgb.train(params, d_train, 1000000, \n",
    "                 valid_sets = d_val, verbose_eval=1000, early_stopping_rounds = 1000)\n",
    "predlg = clfl.predict(X_test)\n",
    "roc_auc_score(y_test, predlg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "result5 = clfl.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['target'] =  (result1+ result2 + result3 + result4 + result5)/5\n",
    "result.to_csv('submission5.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9050868486352358"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clflr = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                         multi_class='multinomial').fit(X_train, y_train)\n",
    "\n",
    "predlr = clflr.predict_proba(X_test)[:,1] \n",
    "roc_auc_score(y_test, predlr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "result6 = clflr.predict_proba(test)[:,1]\n",
    "result['target'] = (result1 + result2 + result3 + result4 + result5)/5\n",
    "result.to_csv('submission5.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['target'] = result6\n",
    "result.to_csv('submssion6.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "skf_three= StratifiedKFold(n_splits=15, shuffle=False, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = np.zeros(len(X))\n",
    "getVal = np.zeros(len(X))\n",
    "predictions = np.zeros(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[461]\tvalid_0's auc: 0.888889\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2026]\tvalid_0's auc: 0.75\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1854]\tvalid_0's auc: 0.770833\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[21]\tvalid_0's auc: 0.895833\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[968]\tvalid_0's auc: 0.902778\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's auc: 0.732639\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's auc: 0.930556\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1186]\tvalid_0's auc: 0.701389\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.836806\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[645]\tvalid_0's auc: 0.763889\n"
     ]
    }
   ],
   "source": [
    "for fold_, (trn_idx, val_idx) in enumerate(skf_three.split(X1, y)):\n",
    "    X_train, y_train = X1.iloc[trn_idx], y.iloc[trn_idx]\n",
    "    X_val, y_val = X1.iloc[val_idx], y.iloc[val_idx]\n",
    "    d_train = lgb.Dataset(X_train, label=y_train)\n",
    "    d_val = lgb.Dataset(X_val, label = y_val)\n",
    "    \n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "    tempgnb = gnb.predict_proba(X_val)[:,1]\n",
    "    tempgnbtest = gnb.predict_proba(test)[:,1]\n",
    "    \n",
    "    clflr = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                         multi_class='multinomial').fit(X_train, y_train)\n",
    "    clf = SVC(gamma='auto',probability=True)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    clfl = lgb.train(params, d_train, 1000000, \n",
    "                 valid_sets = d_val, verbose_eval=100000, early_stopping_rounds = 1000)\n",
    "    predlg = clfl.predict(X_val)\n",
    "\n",
    "    oof[val_idx] = (clflr.predict_proba(X_val)[:,1] + clf.predict_proba(X_val)[:,1]+predlg + tempgnb)/4\n",
    "    temp = (clflr.predict_proba(test)[:,1] + clf.predict_proba(test)[:,1] + clfl.predict(test)+ tempgnbtest)/4\n",
    "    predictions += temp / skf_three.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9269444444444445"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y, oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['target'] = predictions\n",
    "result.to_csv('submission7.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=0.1, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=0.1, gamma='auto', kernel='rbf',\n",
    "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9331249999999999"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for fold_, (trn_idx, val_idx) in enumerate(skf_three.split(X1, y)):\n",
    "    X_train, y_train = X1.iloc[trn_idx], y.iloc[trn_idx]\n",
    "    X_val, y_val = X1.iloc[val_idx], y.iloc[val_idx]\n",
    "    \n",
    "    clf = SVC(C=0.8, cache_size=200, class_weight='balanced', coef0=0.0,\n",
    "              decision_function_shape='ovr', degree=4, gamma='scale', kernel='rbf',\n",
    "              max_iter=-1, probability=True, random_state=2, shrinking=True,\n",
    "              tol=0.01, verbose=False)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    oof[val_idx] = clf.predict_proba(X_val)[:,1]\n",
    "    temp = clf.predict_proba(test)[:,1]\n",
    "    predictions += temp / skf_three.n_splits\n",
    "roc_auc_score(y, oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['target'] = predictions\n",
    "result.to_csv('submission7.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "tem = X1.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kexu\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X1['mean'] = tem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "tem = test.mean(axis=1)\n",
    "test['mean'] = tem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 56 candidates, totalling 280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9342781129545835\n",
      "Best Parameters: {'C': 0.01, 'class_weight': None, 'penalty': 'l2', 'solver': 'saga'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 280 out of 280 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "log_clf = LogisticRegression(random_state=42)\n",
    "param_grid = {\n",
    "    'class_weight' : ['balanced', None], \n",
    "    'penalty' : ['l2','l1'],  \n",
    "    'C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'solver' : ['liblinear', 'saga'],\n",
    "}\n",
    "# Solver newton-cg supports only l2 penalties\n",
    "# Solver lbfgs supports only l2 penalties\n",
    "# Solver sag supports only l2 penalties\n",
    "\n",
    "grid = GridSearchCV(estimator=log_clf, cv=5, param_grid=param_grid,\n",
    "                    scoring='roc_auc', verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Score: {0}\".format(grid.best_score_))\n",
    "print(\"Best Parameters: {0}\".format(grid.best_params_))\n",
    "\n",
    "best_parameters=grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.01, 'class_weight': None, 'penalty': 'l2', 'solver': 'saga'}"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 504 candidates, totalling 2520 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  60 tasks      | elapsed:    0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9330337690631808\n",
      "Best Parameters: {'C': 0.001, 'class_weight': 'balanced', 'coef0': 0, 'degree': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 2520 out of 2520 | elapsed:    2.5s finished\n",
      "C:\\Users\\kexu\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "svc_clf = SVC(random_state=42)\n",
    "param_grid = {\n",
    "    'class_weight' : ['balanced', None], \n",
    "    'degree' : [0, 0.1,0.01,1,2,1.5],  \n",
    "    'C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'coef0' : [0, 0.1,0.01,1,2,1.5],\n",
    "}\n",
    "# Solver newton-cg supports only l2 penalties\n",
    "# Solver lbfgs supports only l2 penalties\n",
    "# Solver sag supports only l2 penalties\n",
    "\n",
    "grid = GridSearchCV(estimator=svc_clf, cv=5, param_grid=param_grid,\n",
    "                    scoring='roc_auc', verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Score: {0}\".format(grid.best_score_))\n",
    "print(\"Best Parameters: {0}\".format(grid.best_params_))\n",
    "\n",
    "best_parameters1=grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = np.zeros(len(X))\n",
    "getVal = np.zeros(len(X))\n",
    "predictions = np.zeros(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kexu\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kexu\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kexu\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kexu\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kexu\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kexu\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kexu\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kexu\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kexu\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kexu\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kexu\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kexu\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kexu\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kexu\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kexu\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9377083333333334"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for fold_, (trn_idx, val_idx) in enumerate(skf_three.split(X1, y)):\n",
    "    X_train, y_train = X1.iloc[trn_idx], y.iloc[trn_idx]\n",
    "    X_val, y_val = X1.iloc[val_idx], y.iloc[val_idx]\n",
    "    \n",
    "    log_clf = LogisticRegression(**best_parameters,random_state = 2)\n",
    "    log_clf.fit(X_train,y_train)\n",
    "    \n",
    "    svc_clf = SVC(**best_parameters1, random_state = 2,probability=True)\n",
    "    svc_clf.fit(X_train,y_train)\n",
    "    \n",
    "    oof[val_idx] = (log_clf.predict_proba(X_val)[:,1] + svc_clf.predict_proba(X_val)[:,1])/ 2\n",
    "    temp = (log_clf.predict_proba(test)[:,1] + svc_clf.predict_proba(test)[:,1])/ 2\n",
    "    predictions += temp/ skf_three.n_splits\n",
    "roc_auc_score(y, oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['target'] = predictions\n",
    "result.to_csv('submission9.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "private score: 0.746"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['random_state'] = 0\n",
    "params['n_jobs'] = -1\n",
    "params['C'] = 0.2\n",
    "params['penalty'] = 'l1'\n",
    "params['class_weight'] = 'balance'\n",
    "params['solver'] = 'saga'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "clfs=[]\n",
    "folds = RepeatedStratifiedKFold(n_splits = 7, n_repeats = 20, random_state = 0)\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X1, y)):\n",
    "        #print('--- Fold {} started at {}'.format(n_fold, time.ctime()))\n",
    "        \n",
    "        train_x, train_y = X1.iloc[train_idx], y.iloc[train_idx]\n",
    "        valid_x, valid_y = X1.iloc[valid_idx], y.iloc[valid_idx]\n",
    "        \n",
    "        clf = LogisticRegression(**params)\n",
    "        clf.fit(train_x, train_y)\n",
    "        clfs.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.zeros(len(test))\n",
    "for clf in clfs:\n",
    "    predictions += clf.predict_proba(test)[:,1]\n",
    "predictions = predictions / len(clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['target'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250</td>\n",
       "      <td>0.811728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>251</td>\n",
       "      <td>0.650123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>252</td>\n",
       "      <td>0.764850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>253</td>\n",
       "      <td>0.920270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>254</td>\n",
       "      <td>0.632398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>255</td>\n",
       "      <td>0.420590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>256</td>\n",
       "      <td>0.566765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>257</td>\n",
       "      <td>0.230232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>258</td>\n",
       "      <td>0.877212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>259</td>\n",
       "      <td>0.280292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>260</td>\n",
       "      <td>0.634627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>261</td>\n",
       "      <td>0.389979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>262</td>\n",
       "      <td>0.345750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>263</td>\n",
       "      <td>0.853013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>264</td>\n",
       "      <td>0.452093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>265</td>\n",
       "      <td>0.907257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>266</td>\n",
       "      <td>0.662660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>267</td>\n",
       "      <td>0.824663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>268</td>\n",
       "      <td>0.682224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>269</td>\n",
       "      <td>0.594866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>270</td>\n",
       "      <td>0.494003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>271</td>\n",
       "      <td>0.909118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>272</td>\n",
       "      <td>0.691908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>273</td>\n",
       "      <td>0.506811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>274</td>\n",
       "      <td>0.504754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>275</td>\n",
       "      <td>0.878569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>276</td>\n",
       "      <td>0.031156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>277</td>\n",
       "      <td>0.905313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>278</td>\n",
       "      <td>0.799813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>279</td>\n",
       "      <td>0.803195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19720</th>\n",
       "      <td>19970</td>\n",
       "      <td>0.341998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19721</th>\n",
       "      <td>19971</td>\n",
       "      <td>0.928658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19722</th>\n",
       "      <td>19972</td>\n",
       "      <td>0.756946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19723</th>\n",
       "      <td>19973</td>\n",
       "      <td>0.160833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19724</th>\n",
       "      <td>19974</td>\n",
       "      <td>0.754135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19725</th>\n",
       "      <td>19975</td>\n",
       "      <td>0.614495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19726</th>\n",
       "      <td>19976</td>\n",
       "      <td>0.914576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19727</th>\n",
       "      <td>19977</td>\n",
       "      <td>0.636525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19728</th>\n",
       "      <td>19978</td>\n",
       "      <td>0.786270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19729</th>\n",
       "      <td>19979</td>\n",
       "      <td>0.959565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19730</th>\n",
       "      <td>19980</td>\n",
       "      <td>0.381226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19731</th>\n",
       "      <td>19981</td>\n",
       "      <td>0.158232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19732</th>\n",
       "      <td>19982</td>\n",
       "      <td>0.851023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19733</th>\n",
       "      <td>19983</td>\n",
       "      <td>0.547603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19734</th>\n",
       "      <td>19984</td>\n",
       "      <td>0.870961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19735</th>\n",
       "      <td>19985</td>\n",
       "      <td>0.566361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19736</th>\n",
       "      <td>19986</td>\n",
       "      <td>0.893102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19737</th>\n",
       "      <td>19987</td>\n",
       "      <td>0.899165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19738</th>\n",
       "      <td>19988</td>\n",
       "      <td>0.846003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19739</th>\n",
       "      <td>19989</td>\n",
       "      <td>0.538785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19740</th>\n",
       "      <td>19990</td>\n",
       "      <td>0.762961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19741</th>\n",
       "      <td>19991</td>\n",
       "      <td>0.771221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19742</th>\n",
       "      <td>19992</td>\n",
       "      <td>0.706875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19743</th>\n",
       "      <td>19993</td>\n",
       "      <td>0.112895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19744</th>\n",
       "      <td>19994</td>\n",
       "      <td>0.920288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19745</th>\n",
       "      <td>19995</td>\n",
       "      <td>0.564725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19746</th>\n",
       "      <td>19996</td>\n",
       "      <td>0.989433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19747</th>\n",
       "      <td>19997</td>\n",
       "      <td>0.464398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19748</th>\n",
       "      <td>19998</td>\n",
       "      <td>0.919856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19749</th>\n",
       "      <td>19999</td>\n",
       "      <td>0.376256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19750 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    target\n",
       "0        250  0.811728\n",
       "1        251  0.650123\n",
       "2        252  0.764850\n",
       "3        253  0.920270\n",
       "4        254  0.632398\n",
       "5        255  0.420590\n",
       "6        256  0.566765\n",
       "7        257  0.230232\n",
       "8        258  0.877212\n",
       "9        259  0.280292\n",
       "10       260  0.634627\n",
       "11       261  0.389979\n",
       "12       262  0.345750\n",
       "13       263  0.853013\n",
       "14       264  0.452093\n",
       "15       265  0.907257\n",
       "16       266  0.662660\n",
       "17       267  0.824663\n",
       "18       268  0.682224\n",
       "19       269  0.594866\n",
       "20       270  0.494003\n",
       "21       271  0.909118\n",
       "22       272  0.691908\n",
       "23       273  0.506811\n",
       "24       274  0.504754\n",
       "25       275  0.878569\n",
       "26       276  0.031156\n",
       "27       277  0.905313\n",
       "28       278  0.799813\n",
       "29       279  0.803195\n",
       "...      ...       ...\n",
       "19720  19970  0.341998\n",
       "19721  19971  0.928658\n",
       "19722  19972  0.756946\n",
       "19723  19973  0.160833\n",
       "19724  19974  0.754135\n",
       "19725  19975  0.614495\n",
       "19726  19976  0.914576\n",
       "19727  19977  0.636525\n",
       "19728  19978  0.786270\n",
       "19729  19979  0.959565\n",
       "19730  19980  0.381226\n",
       "19731  19981  0.158232\n",
       "19732  19982  0.851023\n",
       "19733  19983  0.547603\n",
       "19734  19984  0.870961\n",
       "19735  19985  0.566361\n",
       "19736  19986  0.893102\n",
       "19737  19987  0.899165\n",
       "19738  19988  0.846003\n",
       "19739  19989  0.538785\n",
       "19740  19990  0.762961\n",
       "19741  19991  0.771221\n",
       "19742  19992  0.706875\n",
       "19743  19993  0.112895\n",
       "19744  19994  0.920288\n",
       "19745  19995  0.564725\n",
       "19746  19996  0.989433\n",
       "19747  19997  0.464398\n",
       "19748  19998  0.919856\n",
       "19749  19999  0.376256\n",
       "\n",
       "[19750 rows x 2 columns]"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('submission10.csv', index= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
